# Databricks notebook source
# MAGIC %md
# MAGIC # Data Exploration and Analyis of SEC Quarterly Filings 
# MAGIC 
# MAGIC **Business case:**  
# MAGIC Exploratory analytics on quarterly filings to determine companes of interest.  
# MAGIC **Problem Statement:**  
# MAGIC All data is stored in Redshift.  Need to utilize additional, advanced analytics to advance insight and reduce load on the cluster.  
# MAGIC **Business solution:**  
# MAGIC Provide analytics platform that can integrate with current data warehouse to provide additional tools and analytics capacity.  Incorporate S3 as a data lake to store different sources of data in a single platform. This allows data scientists / analysis to quickly analyze the data and generate reports to predict market trends and/or make financial decisions.  
# MAGIC **Technical Solution:**  
# MAGIC Spark supports connectivity to any JDBC supported database. Use Databricks as a single platform to pull various sources of data from Redshift, or batch dumps into S3 for further processing. ETL the CSV datasets into efficient Parquet formats for performant processing.  
# MAGIC 
# MAGIC Owner: Kevin  
# MAGIC Runnable: Yes  
# MAGIC Last Tested Spark Version: Spark 2.1

# COMMAND ----------

# dbutils.widgets.text("source", "databricks-kevin/test-input/", "Source Folder")
# dbutils.widgets.text("target", "databricks-kevin/test-output/", "Target Folder")
# dbutils.widgets.text("run_mode", "prod", "Run Mode")


# COMMAND ----------

# MAGIC %md ####Get Notebook Parameters

# COMMAND ----------

import json
source = dbutils.widgets.get("source")
target = dbutils.widgets.get("target")
mode = dbutils.widgets.get("run_mode")
# mode = 'test'
print("source: " + source)
print("target: " + target)
print("mode: " + mode)

# COMMAND ----------

# MAGIC %md ####Read in Env Variables

# COMMAND ----------

# MAGIC %run /Users/kevrasm@databricks.com/env_variables

# COMMAND ----------

# MAGIC %md %md ####Ingest Source Files

# COMMAND ----------

from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import BooleanType, StringType

tag_schema = StructType([
  StructField('tag', StringType(), True),
  StructField('version', StringType(), True),
  StructField('custom', BooleanType(), True),
  StructField('abstract', BooleanType(), True),
  StructField('datatype', StringType(), True),
  StructField('iord', StringType(), True),
  StructField('crdr', StringType(), True),
  StructField('tlabel', StringType(), True),
  StructField('doc', StringType(), True)]
 )


# filePath = 's3a://{0}:{1}@databricks-kevin/test-input/tag.txt'.format(AccessKey, SecretKey)
filePath = 's3a://{0}:{1}@{2}'.format(AccessKey, SecretKey, source)
tag_df = spark.read.option("header","true").option("sep","\t").csv(filePath)
print 'Tag Schema:'
tag_df.printSchema()
tag_df.show(1)


# COMMAND ----------

# MAGIC %md If running in test mode it will writethe data out to S3 into a test location and provide a pointer to the 

# COMMAND ----------

if mode == 'test':
  sqlContext.setConf("spark.sql.parquet.compression.codec", "snappy")
  test_loc = "s3a://{0}:{1}@{2}".format(AccessKey, SecretKey, target)
  tag_df.coalesce(1).write.mode("overwrite").parquet(test_loc)
  values = {"status": "Test", "test_loc": target}
  dbutils.notebook.exit(json.dumps(values))
  

# COMMAND ----------

sqlContext.setConf("spark.sql.parquet.compression.codec", "snappy")
write_loc = "s3a://{0}:{1}@{2}".format(AccessKey, SecretKey, target)
tag_df.coalesce(1).write.mode("overwrite").parquet(write_loc)
values = {"status": "Went to wrong spot", "test_loc": target}
dbutils.notebook.exit(json.dumps(values))

# COMMAND ----------

dbutils.notebook.help()
"Workspace/Users/kevrasm@databricks.com/Codepipeline/test_notebook"
